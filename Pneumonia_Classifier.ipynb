{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pneumonia_Classifier",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rmcnew/Pneumonia_Classifier/blob/master/Pneumonia_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDZqRGr729uL",
        "colab_type": "text"
      },
      "source": [
        "#**Pneumonia Classifer**\n",
        "Richard Scott McNew\n",
        "\n",
        "A02077329\n",
        "\n",
        "CS 6600:  Intelligent Systems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJDh0O2Qa17K",
        "colab_type": "text"
      },
      "source": [
        "##Objective\n",
        "\n",
        "Develop and train an image classification network to predict a pneumonia diagnosis when given a chest X-ray image. Use tools other than tflearn in order to explore and learn how to employ other machine learning tools."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39_Z3RuB3tkl",
        "colab_type": "text"
      },
      "source": [
        "##Motivation\n",
        "Pneumonia is the number one killer of children under 5 years old world-wide, killing about 2 million children every year (Rudan et al., 2008).  It is estimated that pediatric pneumonia kills more children than HIV/AIDS, malaria, and measles combined (Adegbola, 2012).  \n",
        "\n",
        "Practically all of these pediatric pneumonia cases occur in developing countries throughout Southeast Asia and Africa.  Pediatric pneumonia can be caused by both baterical or viral pathogens (Mcluckie, 2009).  Timely and accurate diagnosis and follow-on treatment are essential to ensure the survival of affected children. \n",
        "\n",
        "Chest X-rays are a primary means of diagnosing pneumonia, but rapid interpretation of chest X-rays are frequently not available in the developing nations where pediatric pneumonia is common and mortality rates are high.\n",
        "\n",
        "This Pneumonia Classifier takes a chest X-ray image as input and provides a high-confidence automated diagnosis as to whether the patient has pneumonia or not.\n",
        "\n",
        "![Two Chest X-Rays Compared:  a normal chest X-ray and a pneumonia chest X-ray](https://drive.google.com/uc?export=view&id=14kjhca9G-0et78rcxbX4XEh_oyC-RlBl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oan0AUAtcdVB",
        "colab_type": "text"
      },
      "source": [
        "##Dataset\n",
        "The chest X-ray dataset consists of 5856 chest X-ray images of various dimensions.  Images are labelled as one of two sets: NORMAL or PNEUMONIA.\n",
        "\n",
        "Dataset Source URL: https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia/\n",
        "\n",
        "\n",
        "### Dataset Image Counts\n",
        "![Table that shows the number of images in each dataset subset](https://drive.google.com/uc?export=view&id=1W-QYvSI4d69j5crKpYw-X0tjzg40yI_6)\n",
        "\n",
        "### Dataset Image Preprocessing\n",
        "The dataset images must be preprocessed before they are ready for use in training the convolution networks.  The following preprocessing actions are taken for all of the images before they are processed:\n",
        "* Converted to grayscale\n",
        "* Resized to 450 x 450 dimensions\n",
        "* Numpy array values for each pixel are rescaled to [0, 1] range\n",
        "\n",
        "The Train dataset subset images also have the following transformations randomly applied to compensate for the relatively small number of dataset images (dataset augmentation) and to help prevent overfitting:\n",
        "* Images may be randomly rotated up to plus or minus 10 degrees (this is given by the *rotation_range* parameter)\n",
        "* Images may be randomly translated horizontally or vertically up to 10% of the image width or height (this is given in the *width_shift* and *height_shift* parameters)\n",
        "* Images may be randomly shear transformed up to 5% (this is given in the *shear_range* parameter)\n",
        "* Images may be randomly zoomed to only show up to 20% portion of the original image (this is given in the *zoom_range* parameter)\n",
        "\n",
        "The following images show the how randomly applying the above transformations to training images adds variety to the input data:\n",
        "![Images of cats with transforms applied to give variety to the input training dataset](https://drive.google.com/uc?export=view&id=14sJaDMApd5Po7cIZQsIZK03eGbdydA-5)Image source: https://blog.keras.io/img/imgclf/cat_data_augmentation.png\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aYL1p5Lc5hN",
        "colab_type": "text"
      },
      "source": [
        "##Tooling\n",
        "After some exploration, I choose to use Google Colaboratory as the primary tool to train the Pneumonia Classifier convolution neural networks.  This was largely motivated by the limited hardware that I have available to perform convolution net training.\n",
        "\n",
        "###Google Colaboratory Advantages\n",
        "Google Colaboratory offers a no-cost, moderately powerful, GPU-accelerated virtual machine with more processing power and RAM than my laptop (25 GB RAM  on the virtual machine versus 16 GB RAM on my laptop).  The virtual machine is accessed through a Jupyter Notebook web page with Google additions that make it ideal for machine learning and scientific computing exploration. \n",
        "\n",
        "Google Colaboratory enabled me to train the Pneumonia Classifier convolution neural networks much faster and get feedback on my design decisions more rapidly than I could with my laptop.  This screenshot shows a training run with the same dataset and convolution neural network hyperparameters running on a Google Colaboratory instance and my laptop.  (The Google Colaboratory instance is above.  My laptop is below.)\n",
        "\n",
        "![Screenshot showing Colaboratry instance running compared to my laptop running.  Colaboratory training run estimates 1 hour and 4 minutes left to finish the training while my laptop shows 5 hours and 38 minutes left to finish the training.](https://drive.google.com/uc?export=view&id=14i6Tys9WkG8B2Akn5iw9zDIU1uUS4N8R)\n",
        "\n",
        "Note that the Colaboratory instance is completing training epoch 1 much faster than my laptop.  The Colaboratory instance estimates one hour and four minutes left compared to the five hours and thirty-eight minutes left on my laptop.  This is due to a faster virtual machine processor, more RAM, and GPU-acceleration.\n",
        "\n",
        "Google Colaboratory offers a ready-to-go Python-powered machine learning platform based on Jupyter Notebook that requires practically no setup and runs in the web browser.  The pre-installed libraries are Tensorflow-centric, but also include popular Python numerical (numpy, scipy), image processing (opencv, scikit-image), graph visualization (matplotlib, plotly), and machine learning (tensorflow, keras, tflearn, pandas, scikit-learn, pytorch) libraries.  This makes it possible to do a wide range of data transformation and visualization without extensive setup. \n",
        "\n",
        "###Google Colaboratory Disadvantages\n",
        "Google Colaboratory instances on the no-cost tier are intended primarily for interactive use and have a timeout of 90 minutes, meaning that the Colaboratory notebook will be automatically disconnected from a backend virtual machine if no activity occurs in the notebook.  This effectively prevents brute-force searches of the hyperparameters since it means that long-running training of multiple epochs is more difficult to accomplish without \"baby-sitting\" the notebook.  I found this to not be too disadvantageous since it encouraged me to monitor the training more closely and notice immediately when signs of overfitting began to be manifest.\n",
        "\n",
        "Although it does not occur very often, sometimes the Google Colaboratory virtual machine instances can be flaky and will exhibit transitory failures.  This usually manifests as unexpected out-of-memory failures or much slower training times.  In these cases, it was best to either restart the runtime (\"Runtime\" -> \"Restart Runtime\" in the Menu) or disconnect from the \"lemon\" virtual machine backend and let the session timeout.  Following the restart and connecting to a different virtual machine backend the problems disappear and no changes are needed to the code being run.  If \"Out of Memory\" errors continue to occur, this usually means that the model or dataset input data is too large and needs to be scaled down.\n",
        "\n",
        "###Machine Learning Framework Choice\n",
        "The Keras APIs that are included with Tensorflow 2.X are used for image processing, model creation, and model training.  Tensorflow 2.x and Keras are well-supported on Google Colaboratory and take can advantage of GPU acceleration for rapid model training. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVNjS_OoJFPN",
        "colab_type": "text"
      },
      "source": [
        "## Convolution Neural Network Architecture\n",
        "\n",
        "I studied the architecture of several high performance image classification convolution neural networks to find ideas about how to design a suitable convolution network that would be effective a pneumonia classification and still be able to train on the Colaboratory virtual machine and ideally run on commodity hardware.\n",
        "\n",
        "Many of the high performance image classification convolution neural networks (VGG16, VGG19, Xception, and many others) use several convolution layers followed by a single max pooling layer.  I decided to follow a similar design and iterated until I found designs that were a good match for the chest X-ray image dataset and be able to train without encountering \"Out of Memory\" errors.  The following diagram shows the final convolution neural network architecture that I arrived at after several iterations.  The corresponding model code is given in the *create_model* function below.\n",
        "\n",
        "![Pneumonia Classifier convolution neural network architecture diagram](https://drive.google.com/uc?export=view&id=11TznUVyOTkLU2PBEBw1kQrrP657Ni96W) Diagram created using [NN-SVG](http://alexlenail.me/NN-SVG/index.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLzXiRilcpTS",
        "colab_type": "text"
      },
      "source": [
        "##Code for Training and Testing the Convolution Nets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcAn-1X9Uppw",
        "colab_type": "code",
        "outputId": "7a4127d1-ac0b-4326-cddd-c3d1ed5f6ff6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "# Richard Scott McNew\n",
        "# A02077329\n",
        "# CS 6600: Intelligent Systems\n",
        "\n",
        "# use tensorflow 2.x\n",
        "%tensorflow_version 2.x\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "from google.colab import drive, files\n",
        "import datetime\n",
        "import io\n",
        "import os\n",
        "import pathlib\n",
        "from time import sleep\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# enable accelerated linear algebra\n",
        "tf.config.optimizer.set_jit(True)\n",
        "# enable tensorflow AUTOTUNE\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "# Load the TensorBoard notebook extension for training metrics graphs\n",
        "%load_ext tensorboard\n",
        "\n",
        "\n",
        "################### Constants #######################\n",
        "BATCH_SIZE = 8  # Use small batches to allow current batch to fit in GPU memory for faster training\n",
        "IMAGE_SIDE = 450 # We have to resize the dataset images to reduce memory consumption\n",
        "SHUFFLE_SIZE = 25 # We shuffle the images to ensure a better fit \n",
        "EPOCHS = 1  # Only run a few epochs at a time since Colaboratory times out without interactive use\n",
        "\n",
        "# ensure we are in the correct working directory\n",
        "os.chdir(\"/content\")\n",
        "\n",
        "######### Dataset Download and Path Construction #####################\n",
        "# Local path to the dataset\n",
        "DATASET_PATH = \"/content/Pneumonia_Classifier/dataset\"\n",
        "\n",
        "# There is a copy of the Pneumonia dataset in my Pneumonia_Classifier GitHub repo\n",
        "# We can clone the 'dataset_only' branch to get a local copy\n",
        "def get_dataset_files_from_github():\n",
        "    if not os.path.isdir(DATASET_PATH):\n",
        "        print(\"Downloading the dataset from GitHub . . .\")\n",
        "        !git clone -b dataset_only https://github.com/rmcnew/Pneumonia_Classifier.git \n",
        "    else:\n",
        "        print(\"Using previously downloaded dataset\")\n",
        "\n",
        "# There is a tarball of the Pneumonia dataset available as a publicly shared link\n",
        "# from my Google Drive account.  This is probably the fastest way to download a \n",
        "# local copy of the dataset since it should be all within Google's networks\n",
        "def get_dataset_files_from_google_drive_shared():\n",
        "    if not os.path.isdir(DATASET_PATH):\n",
        "        print(\"Downloading the dataset from Google Drive shared link . . .\")\n",
        "        !gdown https://drive.google.com/uc?id=1u2_Ap4rOxHuEKnSb5te070skuoXcJTX9\n",
        "        print(\"Download completed!  Untarring the dataset . . .\")\n",
        "        !tar xjf Pneumonia_Classifier_dataset.tar.bz2\n",
        "        print(\"Dataset is ready!\")\n",
        "    else:\n",
        "        print(\"Using previously downloaded dataset\")\n",
        "\n",
        "# Download a local copy of the dataset and then build paths \n",
        "# to the different dataset subsets: 'train', 'test', and 'validate'\n",
        "#get_dataset_files_from_github()\n",
        "get_dataset_files_from_google_drive_shared()\n",
        "dataset = pathlib.Path(DATASET_PATH)\n",
        "test = dataset.joinpath(\"test\")\n",
        "test_count = len(list(test.glob('**/*.jpeg')))\n",
        "train = dataset.joinpath(\"train\")\n",
        "train_count = len(list(train.glob('**/*.jpeg')))\n",
        "validate = dataset.joinpath(\"validate\")\n",
        "validate_count = len(list(validate.glob('**/*.jpeg')))\n",
        "\n",
        "\n",
        "####################### Dataset Preprocessing #########################\n",
        "# The train_image_generator applies random transformations to the \n",
        "# Train dataset subset to provide dataset augmentation since the dataset\n",
        "# is small and we want to avoid overfitting\n",
        "def create_train_image_generator():\n",
        "    train_image_generator = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        rotation_range=10,\n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1, \n",
        "        zoom_range=0.2, \n",
        "        shear_range=0.05,\n",
        "        fill_mode='nearest')\n",
        "    train_data_gen = train_image_generator.flow_from_directory(\n",
        "            batch_size=BATCH_SIZE, \n",
        "            directory=str(train), \n",
        "            shuffle=True, \n",
        "            target_size=(IMAGE_SIDE, IMAGE_SIDE),\n",
        "            color_mode=\"grayscale\",\n",
        "            class_mode='categorical')\n",
        "    return train_data_gen\n",
        "\n",
        "# Prepare images in the Test dataset subset for use\n",
        "def create_test_image_generator():\n",
        "    test_image_generator = ImageDataGenerator(rescale=1./255)\n",
        "    test_data_gen = test_image_generator.flow_from_directory(\n",
        "            batch_size=BATCH_SIZE, \n",
        "            directory=str(test), \n",
        "            target_size=(IMAGE_SIDE, IMAGE_SIDE),\n",
        "            color_mode=\"grayscale\", \n",
        "            class_mode='categorical')\n",
        "    return test_data_gen\n",
        "\n",
        "# Prepare images in the Validate dataset subset for use\n",
        "def create_validate_image_generator():\n",
        "    validate_image_generator = ImageDataGenerator(rescale=1./255)\n",
        "    validate_data_gen = validate_image_generator.flow_from_directory(\n",
        "            batch_size=BATCH_SIZE, \n",
        "            directory=str(validate), \n",
        "            target_size=(IMAGE_SIDE, IMAGE_SIDE),\n",
        "            color_mode=\"grayscale\", \n",
        "            class_mode='categorical')\n",
        "    return validate_data_gen\n",
        "\n",
        "\n",
        "############################ Model Creation, Loading, and Saving ##############################\n",
        "### This model tends to approach the upper limit of available memory of the Colaboratory virtual machine.\n",
        "### Larger models with more layers or greater numbers of features run into \"Out of Memory\" errors\n",
        "def create_model():\n",
        "    model = Sequential([\n",
        "        Conv2D(300, 10, padding='same', activation='relu', kernel_regularizer='l2', \n",
        "               input_shape=(IMAGE_SIDE, IMAGE_SIDE, 1)),\n",
        "        Conv2D(300, 10, padding='same', activation='relu', kernel_regularizer='l2'),\n",
        "        MaxPooling2D(10),\n",
        "        Dropout(0.2),\n",
        "        Conv2D(100, 4, padding='same', activation='relu', kernel_regularizer='l2'),\n",
        "        Conv2D(100, 4, padding='same', activation='relu', kernel_regularizer='l2'),\n",
        "        MaxPooling2D(),\n",
        "        Dropout(0.2),\n",
        "        Conv2D(50, 3, padding='same', activation='relu', kernel_regularizer='l2'),\n",
        "        Conv2D(50, 3, padding='same', activation='relu', kernel_regularizer='l2'),\n",
        "        MaxPooling2D(),\n",
        "        Flatten(),\n",
        "        Dense(300, activation='relu'),\n",
        "        Dense(2, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy']) \n",
        "    return model\n",
        "\n",
        "###### Trained model from my Google Drive shared link #######\n",
        "# There is the trained persisted model that is shared from my Google Drive account.\n",
        "# This model can be downloaded by anyone who has the shared link URL.\n",
        "# The 'gdown' command line tool is used to perform non-interactive downloads.\n",
        "# This is the 'final' trained model that is trained to be as accurate as possible.\n",
        "\n",
        "# Download a copy of the trained model to evaluate it against the Test or Validate\n",
        "# dataset subsets or to predict using an uploaded chest X-ray image\n",
        "def download_trained_model_from_google_drive_shared(download_anyway=False):\n",
        "    MODEL_PATH = \"/content/pneumonia_classifier_model.h5\"\n",
        "    if not os.path.exists(MODEL_PATH) or download_anyway:\n",
        "        print(\"Downloading trained model from Google Drive shared link . . .\")\n",
        "        !gdown https://drive.google.com/uc?id=1cuxOs6RXWNT3eGjqSbGiyMVp_jhBekq8\n",
        "        print(\"Download completed!  Loading the model . . .\")\n",
        "    else:\n",
        "        print(\"Using previously downloaded model\")\n",
        "    model = tf.keras.models.load_model(MODEL_PATH, custom_objects=None, compile=True)\n",
        "    print(\"Model loaded.\")\n",
        "    return model\n",
        "\n",
        "# Save the trained model and then create a web browser dialogue to download it\n",
        "# This can take a bit to run and download since the model occupies about 700 MB on disk\n",
        "def download_trained_model_via_browser(model):\n",
        "    TMP_PATH = '/tmp/pneumonia_classifier_model.h5'\n",
        "    model.save(TMP_PATH, overwrite=True, include_optimizer=True, save_format='h5')\n",
        "    print(\"Preparing model for browser download.  Please wait as it could take a while . . . .\")\n",
        "    sleep(2)  # pause for two seconds for the file to save\n",
        "    files.download(TMP_PATH)\n",
        "\n",
        "\n",
        "########################### Google Drive functions #############################\n",
        "# These functions are only needed if you want to load a trained network from your\n",
        "# Google Drive storage or save a trained network to your Google Drive storage.\n",
        "\n",
        "# Note that these functions will require interactive steps (opening a page in a \n",
        "# web browser and copying / pasting the authoriztaion code) to create OAuth \n",
        "# tokens that will be used to authorize access to your Google Drive account.\n",
        "# The URL to authorize access is given in the output of this cell (at the bottom)\n",
        "# when these functions are called to access Google Drive.\n",
        "\n",
        "# mount Google drive\n",
        "def mount_drive():\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# unmount Google drive\n",
        "def unmount_drive():\n",
        "    drive.flush_and_unmount()\n",
        "\n",
        "# Google Drive path to the saved model\n",
        "# Note that you may encounter an error if this path does not exist in your Google Drive account\n",
        "GOOGLE_DRIVE_MODEL_PATH = \"/content/drive/My Drive/USU/intelligent_systems/Pneumonia_Classifier/pneumonia_classifier_model.h5\"\n",
        "\n",
        "def save_model_to_google_drive(model):\n",
        "    print(\"Saving model to Google Drive\")\n",
        "    mount_drive()\n",
        "    model.save(GOOGLE_DRIVE_MODEL_PATH, overwrite=True, include_optimizer=True, save_format='h5')    \n",
        "    unmount_drive()\n",
        "\n",
        "def load_trained_model_from_google_drive():\n",
        "    print(\"Loading saved model from Google Drive\")\n",
        "    mount_drive()\n",
        "    model = tf.keras.models.load_model(GOOGLE_DRIVE_MODEL_PATH, custom_objects=None, compile=True)    \n",
        "    return model\n",
        "\n",
        "\n",
        "###################### Model Training and Evaluation ##############################\n",
        "# Create the initial version of the model and run some training epochs\n",
        "# After training epochs run, save the trained model to Google Drive or download it\n",
        "def train_model():\n",
        "    print(\"Training model from scratch\")\n",
        "    print(\"Preparing Train and Test dataset subsets\")\n",
        "    train_data_gen = create_train_image_generator()\n",
        "    test_data_gen = create_test_image_generator()\n",
        "    model = create_model()    \n",
        "    log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "    print(\"Training model . . .\")\n",
        "    history = model.fit(\n",
        "        train_data_gen,        \n",
        "        steps_per_epoch=train_count // BATCH_SIZE,\n",
        "        epochs=EPOCHS,\n",
        "        validation_data=test_data_gen,        \n",
        "        validation_steps=test_count // BATCH_SIZE,\n",
        "        callbacks=[tensorboard_callback]\n",
        "    ) \n",
        "    save_model_to_google_drive(model)\n",
        "\n",
        "\n",
        "# Load the previously trained model from Google Drive, run more training \n",
        "# epochs, and then save the more trained model back to Google Drive\n",
        "def train_model_more():\n",
        "    print(\"Training model more\")\n",
        "    print(\"Preparing training and testing datasets\")\n",
        "    train_data_gen = create_train_image_generator()\n",
        "    test_data_gen = create_test_image_generator()\n",
        "    model = load_trained_model_from_google_drive()    \n",
        "    log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "    print(\"Training model more . . .\")\n",
        "    history = model.fit(\n",
        "        train_data_gen,\n",
        "        steps_per_epoch=train_count // BATCH_SIZE,\n",
        "        epochs=EPOCHS,\n",
        "        validation_data=test_data_gen,\n",
        "        validation_steps=test_count // BATCH_SIZE,\n",
        "        callbacks=[tensorboard_callback]\n",
        "    )    \n",
        "    save_model_to_google_drive(model)\n",
        "    \n",
        "# Download the trained model from the Google Drive shared link\n",
        "# and then evaluate the trained model's accuracy against the \n",
        "# Test dataset subset    \n",
        "def test_trained_model():\n",
        "    print(\"Running model against Test dataset subset\")\n",
        "    test_data_gen = create_test_image_generator()\n",
        "    model = download_trained_model_from_google_drive_shared()\n",
        "    model.evaluate(test_data_gen)\n",
        "\n",
        "# Download the trained model from the Google Drive shared link\n",
        "# and then evaluate the trained model's accuracy against the \n",
        "# Validate dataset subset\n",
        "def validate_trained_model():\n",
        "    print(\"Running model against Validate dataset subset\")\n",
        "    validate_data_gen = create_validate_image_generator()\n",
        "    model = download_trained_model_from_google_drive_shared()\n",
        "    model.evaluate(validate_data_gen)\n",
        "\n",
        "\n",
        "############ Main Section ###############\n",
        "# These function calls will need to be \n",
        "# commented out or uncommented depending \n",
        "# on what you want to do \n",
        "\n",
        "#train_model()           # <-- Run to train the model from scratch and save the trained model to YOUR Google Drive\n",
        "#train_model_more()      # <-- Run to load a trained model from YOUR Google Drive and train it more\n",
        "test_trained_model()    # <-- Run to see accuracy against Test dataset subset; MY trained model is used\n",
        "validate_trained_model() # <-- Run to see accuracy against Validate dataset subset; MY trained model is used\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n",
            "Using previously downloaded dataset\n",
            "Running model against Test dataset subset\n",
            "Found 624 images belonging to 2 classes.\n",
            "Downloading trained model from Google Drive shared link . . .\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1cuxOs6RXWNT3eGjqSbGiyMVp_jhBekq8\n",
            "To: /content/pneumonia_classifier_model.h5\n",
            "46.3MB [00:00, 53.4MB/s]\n",
            "Download completed!  Loading the model . . .\n",
            "Model loaded.\n",
            "78/78 [==============================] - 55s 701ms/step - loss: 0.2791 - accuracy: 0.9151\n",
            "Running model against Validate dataset subset\n",
            "Found 16 images belonging to 2 classes.\n",
            "Using previously downloaded model\n",
            "Model loaded.\n",
            "2/2 [==============================] - 2s 1s/step - loss: 0.3412 - accuracy: 0.9375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9CfYE38faJ_",
        "colab_type": "text"
      },
      "source": [
        "#### TensorBoard Graphs\n",
        "TensorBoard graphs can be used to visualize the convolution network training metrics and determine if any changes need to be made.  The graphs are derived from training metric logs which are enabled via *tensorboard_callback* functions above.\n",
        "\n",
        "Running the cells below will build dynamic graphs from the previous training runs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMVw3sIkim3y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Delete training metric logs from previous training runs\n",
        "# These logs are used by TensorBoard to create the training metrics graphs\n",
        "# so DO NOT run this unless you want to delete the old logs.\n",
        "!rm -rf ./logs/ "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNGYB3QiiI5H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run me to see the training metric graphs for the convolution \n",
        "# net training that you just ran.  Note that the logs are created\n",
        "# on the virtual machine backend and are not persisted unless you\n",
        "# do something to keep the training metric logs.\n",
        "\n",
        "# For some reason TensorBoard needs to be run in its own cell, so \n",
        "# this cell is apart from the rest of the convolution net training\n",
        "# code above\n",
        "%tensorboard --logdir logs/fit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2BMq3qvxf5a",
        "colab_type": "text"
      },
      "source": [
        "### Download Non-dataset Chest X-Ray Images to Run Against the Trained Model\n",
        "\n",
        "[other_xrays.zip](https://drive.google.com/open?id=1Zenutog0CmAftzPfyrPbzmLAeGjEuP_e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrP4clbrbIaJ",
        "colab_type": "text"
      },
      "source": [
        "### Upload and Run a Chest X-Ray Image Against the Trained Model\n",
        "Run the following code block to upload a chest X-ray image from your web browser and get a pneumonia prediction.  \n",
        "\n",
        "Use the \"Choose Files\" button in the output pane to select an image file to upload.  If you get the error message, \"Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable.\" then re-run the code block and it should succeed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-1Q2dXWrK9o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "from google.colab import files\n",
        "import os\n",
        "import shutil\n",
        "import tempfile\n",
        "from time import sleep\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "IMAGE_SIDE = 450 # We have to resize the dataset images to reduce memory consumption\n",
        "\n",
        "def download_trained_model_from_google_drive_shared(download_anyway=False):\n",
        "    MODEL_PATH = \"/content/pneumonia_classifier_model.h5\"\n",
        "    if not os.path.exists(MODEL_PATH) or download_anyway:\n",
        "        print(\"Downloading trained model from Google Drive shared link . . .\")\n",
        "        !gdown https://drive.google.com/uc?id=1cuxOs6RXWNT3eGjqSbGiyMVp_jhBekq8        \n",
        "        print(\"Download completed!  Loading the model . . .\")\n",
        "    else:\n",
        "        print(\"Using previously downloaded model\")\n",
        "    model = tf.keras.models.load_model(MODEL_PATH, custom_objects=None, compile=True)\n",
        "    print(\"Model loaded.\")\n",
        "    return model\n",
        "\n",
        "def upload_images_and_save_files():\n",
        "    files.upload()\n",
        "\n",
        "# create an image generator for the uploaded images\n",
        "def create_image_generator(dir_name):\n",
        "    print(\"Creating image generator for images in directory: \", dir_name)\n",
        "    image_generator = ImageDataGenerator(rescale=1./255)\n",
        "    data_gen = image_generator.flow_from_directory(\n",
        "            directory=str(dir_name), \n",
        "            target_size=(IMAGE_SIDE, IMAGE_SIDE),\n",
        "            color_mode=\"grayscale\",\n",
        "            class_mode=None) # class_mode has to be None to indicate there are no data labels\n",
        "    return data_gen\n",
        "  \n",
        "# Round the prediction to a human-readable interpretation\n",
        "def interpret_prediction(prediction):\n",
        "    print(\"Prediction is:  {}\".format(prediction))\n",
        "    pred_list = prediction[0].tolist()\n",
        "    max_index = pred_list.index(max(pred_list))\n",
        "    if max_index == 0:\n",
        "        print(\"NORMAL\")\n",
        "    else:\n",
        "        print(\"PNEUMONIA\")\n",
        "\n",
        "##### Main section #####\n",
        "model = download_trained_model_from_google_drive_shared()\n",
        "# create temporary directory\n",
        "temp_dir_name = tempfile.mkdtemp()\n",
        "print('Created temporary directory: ', temp_dir_name)\n",
        "# cd to temporary directory\n",
        "os.chdir(temp_dir_name)\n",
        "# make subdirectory for image files;  per the Keras documentation:  \n",
        "# \"Please note that in case of class_mode None, the data still needs \n",
        "# to reside in a subdirectory of directory for it to work correctly.\"\n",
        "image_path = os.path.join(temp_dir_name, \"images\")\n",
        "os.mkdir(image_path)\n",
        "os.chdir(image_path)\n",
        "# upload image files from user's web browser into image_path\n",
        "upload_images_and_save_files()\n",
        "sleep(2) # pause to save files to disk\n",
        "# preprocess uploaded images\n",
        "image_gen = create_image_generator(temp_dir_name)\n",
        "# run prediction\n",
        "prediction = model.predict(image_gen)\n",
        "# interpret and print out result\n",
        "interpret_prediction(prediction)\n",
        "# return to starting directory\n",
        "os.chdir(\"/content\")\n",
        "# clean-up temporary directory\n",
        "shutil.rmtree(temp_dir_name)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYKJE3oyXE1I",
        "colab_type": "text"
      },
      "source": [
        "##Performance Results\n",
        "The following table shows how the trained models performed against the Test and Validate dataset subsets:\n",
        "\n",
        "![Pneumonia Classifier convolution neural network performance results](https://drive.google.com/uc?export=view&id=15N8BU9loqGSiHyL7YNrojbEd02zbbXwY)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dE0gScnpupw4",
        "colab_type": "text"
      },
      "source": [
        "##Download Trained Model\n",
        "The final trained model is available on GitHub:  https://github.com/rmcnew/Pneumonia_Classifier/raw/master/pneumonia_classifier_model.h5\n",
        "\n",
        "Or you can run this cell to download the trained model from my Google Drive shared link.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miPFGzI5tDI6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###### Trained model from my Google Drive shared link #######\n",
        "# There is the trained persisted model that is shared from my Google Drive account.\n",
        "# This model can be downloaded by anyone who has the shared link URL.\n",
        "# The 'gdown' command line tool is used to perform non-interactive downloads.\n",
        "# This is the 'final' trained model that is trained to be as accurate as possible.\n",
        "\n",
        "# Download a copy of the trained model to evaluate it against the Test or Validate\n",
        "# dataset subsets or to predict using an uploaded chest X-ray image\n",
        "def download_trained_model_from_google_drive_shared(download_anyway=False):\n",
        "    MODEL_PATH = \"/content/pneumonia_classifier_model.h5\"\n",
        "    if not os.path.exists(MODEL_PATH) or download_anyway:\n",
        "        print(\"Downloading trained model from Google Drive shared link . . .\")\n",
        "        !gdown https://drive.google.com/uc?id=1cuxOs6RXWNT3eGjqSbGiyMVp_jhBekq8        \n",
        "        print(\"Download completed!  Loading the model . . .\")\n",
        "    else:\n",
        "        print(\"Using previously downloaded model\")\n",
        "    model = tf.keras.models.load_model(MODEL_PATH, custom_objects=None, compile=True)\n",
        "    print(\"Model loaded.\")\n",
        "    return model\n",
        "\n",
        "# Save the trained model and then create a web browser dialogue to download it\n",
        "# This can take a bit to run and download since the model occupies about 700 MB on disk\n",
        "def download_trained_model_via_browser(model):\n",
        "    TMP_PATH = '/tmp/pneumonia_classifier_model.h5'\n",
        "    model.save(TMP_PATH, overwrite=True, include_optimizer=True, save_format='h5')\n",
        "    print(\"Preparing model for browser download.  Please wait as it could take a while . . . .\")\n",
        "    sleep(2)  # pause for two seconds for the file to save\n",
        "    files.download(TMP_PATH)\n",
        "\n",
        "# Main section\n",
        "model = download_trained_model_from_google_drive_shared(download_anyway=True)\n",
        "download_trained_model_via_browser(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKi9fuq0gd_1",
        "colab_type": "text"
      },
      "source": [
        "## Ideas for Fielding the Pneumonia Classifier\n",
        "\n",
        "The trained Pneumonia Classifier convolution network is about 45 megabytes on disk and uses less than a gigabyte of RAM when run under Python.  This means that it can run on commodity hardware and deployed for field use as part of a mobile radiography lab to assist in \"on-the-spot\" diagnoses.  For example, it could easily be included on the radiography laptop computer that is a component of the Mobile Diagnostic Unit described by Dhoot et al (Dhoot R, et al., 2018).\n",
        "\n",
        "![Mobile X-Ray Laboratory mounted on the back of a truck.](https://drive.google.com/uc?export=view&id=1wmYocqu4ZFtFJx89GOZQRLDmUBWbuIwx)Image source: https://gh.bmj.com/content/bmjgh/3/5/e000947.full.pdf\n",
        "\n",
        "The Pneumonia Classifier would help to fill a critical shortage in the rapid interpretation of chest X-Ray results.  Dhoot et al state: \"A primary radiologist reading, which is the standard of care throughout the USA, is challenging in\n",
        "Kenya due to multiple factors including poor internet\n",
        "connectivity, too few radiologists and delivery time for\n",
        "paper reports. Poor internet connectivity in rural areas\n",
        "creates delays in archiving images for radiologist review\n",
        "and precludes clinicians’ access to the online results\n",
        "portal.\"  If the Pneumonia Classifier were to be deployed with the Mobile Diagnostic Unit or a similar mobile field radiography system it could provide an immediate preliminary field screening that would help patients get the care they need more rapidly.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_mLanki7CHJ",
        "colab_type": "text"
      },
      "source": [
        "##References\n",
        "Kermany et al., 2018.  \"Identifying Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning\", Cell 172, p. 1122–1131, February 22, 2018.  Elsevier Inc.\n",
        "https://doi.org/10.1016/j.cell.2018.02.010\n",
        "\n",
        "\n",
        "Rudan, I., Boschi-Pinto, C., Biloglav, Z., Mulholland, K., and Campbell, H. (2008). \"Epidemiology and Etiology of Childhood Pneumonia\" Bulletin of the World Health Organization 86, 408–416.\n",
        "\n",
        "\n",
        "Adegbola, R.A. (2012). \"Childhood Pneumonia as a Global Health Priority and the Strategic Interest of the Bill & Melinda Gates Foundation\". Clinical Infectious Diseases 54\n",
        "(Supplement 2), S89–S92.\n",
        "\n",
        "\n",
        "Mcluckie, A. (2009). \"Respiratory disease and its management\", Volume 57 (Springer).\n",
        "\n",
        "Dhoot R, et al. (2018). \"Implementing a Mobile Diagnostic Unit\n",
        "to Increase Access to Imaging and Laboratory Services in Western Kenya\" BMJ Glob Health 2018; 3:e000947. doi:10.1136/bmjgh-2018-000947  Available at https://gh.bmj.com/content/bmjgh/3/5/e000947.full.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPIPURBBqV05",
        "colab_type": "text"
      },
      "source": [
        "## License\n",
        "\n",
        "*Pneumonia_Classifier:  a machine learning image classifier for pediatric chest X-rays to help detect pneumonia*\n",
        "\n",
        "Copyright (C) 2019  Richard Scott McNew.\n",
        "\n",
        "This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.\n",
        "\n",
        "This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.\n",
        "\n",
        "You should have received a copy of the GNU General Public License along with this program.  If not, see <https://www.gnu.org/licenses/>."
      ]
    }
  ]
}