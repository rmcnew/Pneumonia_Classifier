{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pneumonia_Classifier",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rmcnew/Pneumonia_Classifier/blob/master/Pneumonia_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDZqRGr729uL",
        "colab_type": "text"
      },
      "source": [
        "# Pneumonia Classifer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcAn-1X9Uppw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "83470dc5-4ad5-4df7-c724-9463d5d27cb1"
      },
      "source": [
        "# Richard Scott McNew\n",
        "# A02077329\n",
        "# CS 6600: Intelligent Systems\n",
        "\n",
        "# use tensorflow 2.x\n",
        "%tensorflow_version 2.x\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "from google.colab import drive, files\n",
        "import datetime\n",
        "import io\n",
        "import os\n",
        "import pathlib\n",
        "from time import sleep\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# enable accelerated linear algebra\n",
        "tf.config.optimizer.set_jit(True)\n",
        "# enable tensorflow AUTOTUNE\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "# Load the TensorBoard notebook extension for training metrics graphs\n",
        "%load_ext tensorboard\n",
        "\n",
        "\n",
        "################### Constants #######################\n",
        "BATCH_SIZE = 8  # Use small batches to allow current batch to fit in GPU memory for faster training\n",
        "IMAGE_SIDE = 450 \n",
        "SHUFFLE_SIZE = 25\n",
        "EPOCHS = 2  # Only run a few epochs at a time since Colaboratory times out \n",
        "\n",
        "\n",
        "######### Dataset Download and Path Construction #####################\n",
        "# Local path to the dataset\n",
        "DATASET_PATH = \"/content/Pneumonia_Classifier/dataset\"\n",
        "\n",
        "# There is a copy of the Pneumonia dataset in my Pneumonia_Classifier GitHub repo\n",
        "# We can clone the 'dataset_only' branch to get a local copy\n",
        "def get_dataset_files_from_github():\n",
        "    if not os.path.isdir(DATASET_PATH):\n",
        "        print(\"Downloading the dataset from GitHub . . .\")\n",
        "        !git clone -b dataset_only https://github.com/rmcnew/Pneumonia_Classifier.git \n",
        "    else:\n",
        "        print(\"Using previously downloaded dataset\")\n",
        "\n",
        "# There is a tarball of the Pneumonia dataset available as a publicly shared link\n",
        "# from my Google Drive account.  This is probably the fastest way to download a \n",
        "# local copy of the dataset since it should be all within Google's networks\n",
        "def get_dataset_files_from_google_drive_shared():\n",
        "    if not os.path.isdir(DATASET_PATH):\n",
        "        print(\"Downloading the dataset from Google Drive shared link . . .\")\n",
        "        !gdown https://drive.google.com/uc?id=1u2_Ap4rOxHuEKnSb5te070skuoXcJTX9\n",
        "        !tar xjf Pneumonia_Classifier_dataset.tar.bz2\n",
        "        print(\"Download completed!  Untarring the model . . .\")\n",
        "    else:\n",
        "        print(\"Using previously downloaded dataset\")\n",
        "\n",
        "# Download a local copy of the dataset and then build paths \n",
        "# to the different dataset subsets: 'train', 'test', and 'validate'\n",
        "#get_dataset_files_from_github()\n",
        "get_dataset_files_from_google_drive_shared()\n",
        "dataset = pathlib.Path(DATASET_PATH)\n",
        "test = dataset.joinpath(\"test\")\n",
        "test_count = len(list(test.glob('**/*.jpeg')))\n",
        "train = dataset.joinpath(\"train\")\n",
        "train_count = len(list(train.glob('**/*.jpeg')))\n",
        "validate = dataset.joinpath(\"validate\")\n",
        "validate_count = len(list(validate.glob('**/*.jpeg')))\n",
        "\n",
        "\n",
        "####################### Dataset Preprocessing #########################\n",
        "def create_train_image_generator():\n",
        "    train_image_generator = ImageDataGenerator(rescale=1./255, zoom_range=0.5)\n",
        "    train_data_gen = train_image_generator.flow_from_directory(\n",
        "            batch_size=BATCH_SIZE, \n",
        "            directory=str(train), \n",
        "            shuffle=True, \n",
        "            target_size=(IMAGE_SIDE, IMAGE_SIDE), \n",
        "            class_mode='binary')\n",
        "    return train_data_gen\n",
        "\n",
        "def create_test_image_generator():\n",
        "    test_image_generator = ImageDataGenerator(rescale=1./255)\n",
        "    test_data_gen = test_image_generator.flow_from_directory(\n",
        "            batch_size=BATCH_SIZE, \n",
        "            directory=str(test), \n",
        "            target_size=(IMAGE_SIDE, IMAGE_SIDE), \n",
        "            class_mode='binary')\n",
        "    return test_data_gen\n",
        "\n",
        "def create_validate_image_generator():\n",
        "    validate_image_generator = ImageDataGenerator(rescale=1./255)\n",
        "    validate_data_gen = validate_image_generator.flow_from_directory(\n",
        "            batch_size=BATCH_SIZE, \n",
        "            directory=str(validate), \n",
        "            target_size=(IMAGE_SIDE, IMAGE_SIDE), \n",
        "            class_mode='binary')\n",
        "    return validate_data_gen\n",
        "\n",
        "\n",
        "############################ Model Creation, Loading, and Saving ##############################\n",
        "\n",
        "# val 90\n",
        "#def create_model():\n",
        "#    model = Sequential([\n",
        "#        Conv2D(450, 10, padding='same', activation='relu', kernel_regularizer='l2', input_shape=(450, 450, 3)),\n",
        "#        MaxPooling2D(),\n",
        "#        Conv2D(225, 5, padding='same', activation='relu', kernel_regularizer='l2'),\n",
        "#        MaxPooling2D(),\n",
        "#        Dropout(0.2),\n",
        "#        Conv2D(100, 5, padding='same', activation='relu', kernel_regularizer='l2'),\n",
        "#        MaxPooling2D(),        \n",
        "#        Flatten(),\n",
        "#        Dense(512, activation='relu'),\n",
        "#        Dense(1, activation='sigmoid')\n",
        "#    ])\n",
        "#    model.compile(optimizer='SGD', loss='binary_crossentropy', metrics=['accuracy']) \n",
        "#    return model\n",
        "\n",
        "\n",
        "def create_model():\n",
        "    model = Sequential([\n",
        "        Conv2D(IMAGE_SIDE, 10, padding='same', activation='relu', kernel_regularizer='l2', input_shape=(IMAGE_SIDE, IMAGE_SIDE, 3)),\n",
        "        MaxPooling2D(),\n",
        "        Conv2D(225, 5, padding='same', activation='relu', kernel_regularizer='l2'),\n",
        "        MaxPooling2D(),\n",
        "        Dropout(0.2),\n",
        "        Conv2D(100, 5, padding='same', activation='relu', kernel_regularizer='l2'),\n",
        "        MaxPooling2D(),        \n",
        "        Flatten(),\n",
        "        Dense(512, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='SGD', loss='binary_crossentropy', metrics=['accuracy']) \n",
        "    return model\n",
        "\n",
        "    \n",
        "# There is a trained persisted model that is shared from my Google Drive account\n",
        "# Download a copy to local storage for use in evaluating against the datasets\n",
        "def download_trained_model_from_google_drive_shared(download_anyway=False):\n",
        "    MODEL_PATH = \"/content/pneumonia_classifier_model.h5\"\n",
        "    if not os.path.exists(MODEL_PATH) or download_anyway:\n",
        "        print(\"Downloading trained model from Google Drive shared link . . .\")\n",
        "        !gdown https://drive.google.com/uc?id=1Qwm5QlveZsUzO6vU7jMBUOgx_uHlSN9i\n",
        "        print(\"Download completed!  Loading the model . . .\")\n",
        "    else:\n",
        "        print(\"Using previously downloaded model\")\n",
        "    model = tf.keras.models.load_model(MODEL_PATH, custom_objects=None, compile=True)\n",
        "    print(\"Model loaded.\")\n",
        "    return model\n",
        "\n",
        "# Save the trained model and then create a web browser dialogue to download it\n",
        "# This can take a bit to run and download since the model occupies about 700 MB on disk\n",
        "def download_trained_model_via_browser(model):\n",
        "    TMP_PATH = '/tmp/pneumonia_classifier_model.h5'\n",
        "    model.save(TMP_PATH, overwrite=True, include_optimizer=True, save_format='h5')\n",
        "    print(\"Preparing model for browser download.  Please wait as it could take a while . . . .\")\n",
        "    sleep(2)  # pause for two seconds for the file to save\n",
        "    files.download(TMP_PATH)\n",
        "\n",
        "########################### Google Drive functions #############################\n",
        "# Note that these functions will require interactive steps (opening a page in a \n",
        "# web browser and copying / pasting the authoriztaion code) to create OAuth \n",
        "# tokens that will be used to authorize access to your Google Drive account\n",
        "\n",
        "# Google Drive path to the saved model\n",
        "GOOGLE_DRIVE_MODEL_PATH = \"/content/drive/My Drive/USU/intelligent_systems/Pneumonia_Classifier/pneumonia_classifier_model.h5\"\n",
        "\n",
        "# mount Google drive\n",
        "def mount_drive():\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# unmount Google drive\n",
        "def unmount_drive():\n",
        "    drive.flush_and_unmount()\n",
        "\n",
        "def save_model_to_google_drive(model):\n",
        "    mount_drive()\n",
        "    model.save(GOOGLE_DRIVE_MODEL_PATH, overwrite=True, include_optimizer=True, save_format='h5')\n",
        "    unmount_drive()\n",
        "\n",
        "def load_trained_model_from_google_drive():\n",
        "    mount_drive()\n",
        "    model = tf.keras.models.load_model(GOOGLE_DRIVE_MODEL_PATH, custom_objects=None, compile=True)\n",
        "    unmount_drive()\n",
        "    return model\n",
        "\n",
        "\n",
        "###################### Model Training and Evaluation ##############################\n",
        "def train_model():\n",
        "    train_data_gen = create_train_image_generator()\n",
        "    test_data_gen = create_test_image_generator()\n",
        "    model = create_model()\n",
        "    log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "    history = model.fit(\n",
        "        train_data_gen,        \n",
        "        steps_per_epoch=train_count // BATCH_SIZE,\n",
        "        epochs=EPOCHS,\n",
        "        validation_data=test_data_gen,        \n",
        "        validation_steps=test_count // BATCH_SIZE,\n",
        "        callbacks=[tensorboard_callback]\n",
        "    ) \n",
        "    save_model_to_google_drive(model)\n",
        "    %tensorboard --logdir logs/fit    \n",
        "\n",
        "\n",
        "def train_model_more():\n",
        "    train_data_gen = create_train_image_generator()\n",
        "    test_data_gen = create_test_image_generator()\n",
        "    model = load_trained_model_from_google_drive()    \n",
        "    log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "    history = model.fit(\n",
        "        train_data_gen,\n",
        "        steps_per_epoch=train_count // BATCH_SIZE,\n",
        "        epochs=EPOCHS,\n",
        "        validation_data=test_data_gen,\n",
        "        validation_steps=test_count // BATCH_SIZE,\n",
        "        callbacks=[tensorboard_callback]\n",
        "    )    \n",
        "    save_model_to_google_drive(model)\n",
        "    %tensorboard --logdir logs/fit\n",
        "    \n",
        "    \n",
        "def test_trained_model():\n",
        "    test_data_gen = create_test_image_generator()\n",
        "    model = download_trained_model_from_google_drive_shared()\n",
        "    model.evaluate(test_data_gen)\n",
        "\n",
        "def validate_trained_model():\n",
        "    validate_data_gen = create_validate_image_generator()\n",
        "    model = download_trained_model_from_google_drive_shared()\n",
        "    model.evaluate(validate_data_gen)\n",
        "\n",
        "############ Main Section ###############\n",
        "\n",
        "#test_trained_model()\n",
        "train_model_more()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n",
            "Downloading the dataset from Google Drive shared link . . .\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1u2_Ap4rOxHuEKnSb5te070skuoXcJTX9\n",
            "To: /content/Pneumonia_Classifier_dataset.tar.bz2\n",
            "1.19GB [00:06, 172MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}