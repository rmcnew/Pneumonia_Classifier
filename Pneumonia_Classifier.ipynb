{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pneumonia_Classifier",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rmcnew/Pneumonia_Classifier/blob/master/Pneumonia_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hfkLTSImBHo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 list "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcAn-1X9Uppw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Richard Scott McNew\n",
        "# A02077329\n",
        "# CS 6600: Intelligent Systems\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "from google.colab import drive\n",
        "import os\n",
        "import pathlib\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# enable accelerated linear algebra\n",
        "tf.config.optimizer.set_jit(True)\n",
        "# enable tensorflow AUTOTUNE\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "IMAGE_SIDE = 450 \n",
        "SHUFFLE_SIZE = 25\n",
        "EPOCHS = 2\n",
        "MODEL_PATH = \"/content/drive/My Drive/USU/intelligent_systems/Pneumonia_Classifier/pneumonia_classifier_model\"\n",
        "DATASET_PATH = \"/content/drive/My Drive/USU/intelligent_systems/Pneumonia_Classifier/dataset\"\n",
        "\n",
        "# mount drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# dataset paths\n",
        "dataset = pathlib.Path(DATASET_PATH)\n",
        "test = dataset.joinpath(\"test\")\n",
        "test_count = len(list(test.glob('**/*.jpeg')))\n",
        "train = dataset.joinpath(\"train\")\n",
        "train_count = len(list(train.glob('**/*.jpeg')))\n",
        "validate = dataset.joinpath(\"validate\")\n",
        "validate_count = len(list(validate.glob('**/*.jpeg')))\n",
        "\n",
        "\n",
        "def create_train_image_generator():\n",
        "    train_image_generator = ImageDataGenerator(rescale=1./255, zoom_range=0.5)\n",
        "    train_data_gen = train_image_generator.flow_from_directory(\n",
        "            batch_size=BATCH_SIZE, \n",
        "            directory=str(train), \n",
        "            shuffle=True, \n",
        "            target_size=(IMAGE_SIDE, IMAGE_SIDE), \n",
        "            class_mode='binary')\n",
        "    return train_data_gen\n",
        "\n",
        "def create_test_image_generator():\n",
        "    test_image_generator = ImageDataGenerator(rescale=1./255)\n",
        "    test_data_gen = test_image_generator.flow_from_directory(\n",
        "            batch_size=BATCH_SIZE, \n",
        "            directory=str(test), \n",
        "            target_size=(IMAGE_SIDE, IMAGE_SIDE), \n",
        "            class_mode='binary')\n",
        "    return test_data_gen\n",
        "\n",
        "def create_validate_image_generator():\n",
        "    validate_image_generator = ImageDataGenerator(rescale=1./255)\n",
        "    validate_data_gen = validate_image_generator.flow_from_directory(\n",
        "            batch_size=BATCH_SIZE, \n",
        "            directory=str(validate), \n",
        "            target_size=(IMAGE_SIDE, IMAGE_SIDE), \n",
        "            class_mode='binary')\n",
        "    return validate_data_gen\n",
        "\n",
        "# val 90\n",
        "#def create_model():\n",
        "#    model = Sequential([\n",
        "#        Conv2D(450, 10, padding='same', activation='relu', kernel_regularizer='l2', input_shape=(450, 450, 3)),\n",
        "#        MaxPooling2D(),\n",
        "#        Conv2D(225, 5, padding='same', activation='relu', kernel_regularizer='l2'),\n",
        "#        MaxPooling2D(),\n",
        "#        Dropout(0.2),\n",
        "#        Conv2D(100, 5, padding='same', activation='relu', kernel_regularizer='l2'),\n",
        "#        MaxPooling2D(),        \n",
        "#        Flatten(),\n",
        "#        Dense(512, activation='relu'),\n",
        "#        Dense(1, activation='sigmoid')\n",
        "#    ])\n",
        "#    model.compile(optimizer='SGD', loss='binary_crossentropy', metrics=['accuracy']) \n",
        "#    return model\n",
        "\n",
        "\n",
        "def create_model():\n",
        "    model = Sequential([\n",
        "        Conv2D(IMAGE_SIDE, 10, padding='same', activation='relu', kernel_regularizer='l2', input_shape=(IMAGE_SIDE, IMAGE_SIDE, 3)),\n",
        "        MaxPooling2D(),\n",
        "        Conv2D(225, 5, padding='same', activation='relu', kernel_regularizer='l2'),\n",
        "        MaxPooling2D(),\n",
        "        Dropout(0.2),\n",
        "        Conv2D(100, 5, padding='same', activation='relu', kernel_regularizer='l2'),\n",
        "        MaxPooling2D(),        \n",
        "        Flatten(),\n",
        "        Dense(512, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='SGD', loss='binary_crossentropy', metrics=['accuracy']) \n",
        "    return model\n",
        "\n",
        "def train_model():\n",
        "    train_data_gen = create_train_image_generator()\n",
        "    test_data_gen = create_test_image_generator()\n",
        "    model = create_model()\n",
        "    model.fit(\n",
        "        train_data_gen,        \n",
        "        steps_per_epoch=train_count // BATCH_SIZE,\n",
        "        epochs=EPOCHS,\n",
        "        validation_data=test_data_gen,        \n",
        "        validation_steps=test_count // BATCH_SIZE\n",
        "    ) \n",
        "    model.save(MODEL_PATH)\n",
        "    \n",
        "\n",
        "def unmount_drive():\n",
        "  drive.flush_and_unmount()\n",
        "\n",
        "def load_model(path):\n",
        "    model = tf.keras.models.load_model(path, custom_objects=None, compile=True)\n",
        "    return model\n",
        "\n",
        "def load_trained_model():\n",
        "    model = tf.keras.models.load_model(MODEL_PATH, custom_objects=None, compile=True)\n",
        "    return model\n",
        "\n",
        "def train_model_more():\n",
        "    train_data_gen = create_train_image_generator()\n",
        "    test_data_gen = create_test_image_generator()\n",
        "    model = load_trained_model()\n",
        "    model.fit(\n",
        "        train_data_gen,\n",
        "        steps_per_epoch=train_count // BATCH_SIZE,\n",
        "        epochs=EPOCHS,\n",
        "        validation_data=test_data_gen,\n",
        "        validation_steps=test_count // BATCH_SIZE\n",
        "    )\n",
        "    model.save(MODEL_PATH)\n",
        "    \n",
        "\n",
        "train_model_more()\n",
        "unmount_drive()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}