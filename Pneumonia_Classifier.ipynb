{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pneumonia_Classifier",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rmcnew/Pneumonia_Classifier/blob/master/Pneumonia_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDZqRGr729uL",
        "colab_type": "text"
      },
      "source": [
        "# Pneumonia Classifer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcAn-1X9Uppw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Richard Scott McNew\n",
        "# A02077329\n",
        "# CS 6600: Intelligent Systems\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "from google.colab import drive, files\n",
        "import io\n",
        "import os\n",
        "import pathlib\n",
        "from time import sleep\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# enable accelerated linear algebra\n",
        "tf.config.optimizer.set_jit(True)\n",
        "# enable tensorflow AUTOTUNE\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "IMAGE_SIDE = 450 \n",
        "SHUFFLE_SIZE = 25\n",
        "EPOCHS = 2\n",
        "\n",
        "\n",
        "DATASET_PATH = \"/content/Pneumonia_Classifier/dataset\"\n",
        "\n",
        "def get_dataset_files():\n",
        "    if not os.path.isdir(DATASET_PATH):\n",
        "        print(\"Downloading the dataset . . .\")\n",
        "        !git clone -b dataset_only https://github.com/rmcnew/Pneumonia_Classifier.git \n",
        "    else:\n",
        "        print(\"Using previously downloaded dataset\")\n",
        "\n",
        "\n",
        "# dataset paths\n",
        "get_dataset_files()\n",
        "dataset = pathlib.Path(DATASET_PATH)\n",
        "test = dataset.joinpath(\"test\")\n",
        "test_count = len(list(test.glob('**/*.jpeg')))\n",
        "train = dataset.joinpath(\"train\")\n",
        "train_count = len(list(train.glob('**/*.jpeg')))\n",
        "validate = dataset.joinpath(\"validate\")\n",
        "validate_count = len(list(validate.glob('**/*.jpeg')))\n",
        "\n",
        "####################### Dataset Preprocessing #########################\n",
        "def create_train_image_generator():\n",
        "    train_image_generator = ImageDataGenerator(rescale=1./255, zoom_range=0.5)\n",
        "    train_data_gen = train_image_generator.flow_from_directory(\n",
        "            batch_size=BATCH_SIZE, \n",
        "            directory=str(train), \n",
        "            shuffle=True, \n",
        "            target_size=(IMAGE_SIDE, IMAGE_SIDE), \n",
        "            class_mode='binary')\n",
        "    return train_data_gen\n",
        "\n",
        "def create_test_image_generator():\n",
        "    test_image_generator = ImageDataGenerator(rescale=1./255)\n",
        "    test_data_gen = test_image_generator.flow_from_directory(\n",
        "            batch_size=BATCH_SIZE, \n",
        "            directory=str(test), \n",
        "            target_size=(IMAGE_SIDE, IMAGE_SIDE), \n",
        "            class_mode='binary')\n",
        "    return test_data_gen\n",
        "\n",
        "def create_validate_image_generator():\n",
        "    validate_image_generator = ImageDataGenerator(rescale=1./255)\n",
        "    validate_data_gen = validate_image_generator.flow_from_directory(\n",
        "            batch_size=BATCH_SIZE, \n",
        "            directory=str(validate), \n",
        "            target_size=(IMAGE_SIDE, IMAGE_SIDE), \n",
        "            class_mode='binary')\n",
        "    return validate_data_gen\n",
        "\n",
        "############################ Model Creation and Loading ##############################\n",
        "\n",
        "# val 90\n",
        "#def create_model():\n",
        "#    model = Sequential([\n",
        "#        Conv2D(450, 10, padding='same', activation='relu', kernel_regularizer='l2', input_shape=(450, 450, 3)),\n",
        "#        MaxPooling2D(),\n",
        "#        Conv2D(225, 5, padding='same', activation='relu', kernel_regularizer='l2'),\n",
        "#        MaxPooling2D(),\n",
        "#        Dropout(0.2),\n",
        "#        Conv2D(100, 5, padding='same', activation='relu', kernel_regularizer='l2'),\n",
        "#        MaxPooling2D(),        \n",
        "#        Flatten(),\n",
        "#        Dense(512, activation='relu'),\n",
        "#        Dense(1, activation='sigmoid')\n",
        "#    ])\n",
        "#    model.compile(optimizer='SGD', loss='binary_crossentropy', metrics=['accuracy']) \n",
        "#    return model\n",
        "\n",
        "\n",
        "def create_model():\n",
        "    model = Sequential([\n",
        "        Conv2D(IMAGE_SIDE, 10, padding='same', activation='relu', kernel_regularizer='l2', input_shape=(IMAGE_SIDE, IMAGE_SIDE, 3)),\n",
        "        MaxPooling2D(),\n",
        "        Conv2D(225, 5, padding='same', activation='relu', kernel_regularizer='l2'),\n",
        "        MaxPooling2D(),\n",
        "        Dropout(0.2),\n",
        "        Conv2D(100, 5, padding='same', activation='relu', kernel_regularizer='l2'),\n",
        "        MaxPooling2D(),        \n",
        "        Flatten(),\n",
        "        Dense(512, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='SGD', loss='binary_crossentropy', metrics=['accuracy']) \n",
        "    return model\n",
        "\n",
        "    \n",
        "# There is a trained persisted model that is shared from my Google Drive account\n",
        "# Download a copy to local storage for use in evaluating against the datasets\n",
        "def download_trained_model_from_google_drive_shared(download_anyway=False):\n",
        "    MODEL_PATH = \"/content/pneumonia_classifier_model\"\n",
        "    if not os.path.exists(MODEL_PATH) or download_anyway:\n",
        "        print(\"Downloading trained model from Google Drive . . .\")\n",
        "        !gdown https://drive.google.com/uc?id=1Qwm5QlveZsUzO6vU7jMBUOgx_uHlSN9i\n",
        "        print(\"Download completed!  Loading the model . . .\")\n",
        "    else:\n",
        "        print(\"Using previously downloaded model\")\n",
        "    model = tf.keras.models.load_model(MODEL_PATH, custom_objects=None, compile=True)\n",
        "    print(\"Model loaded.\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def download_trained_model_via_browser(model):\n",
        "    TMP_PATH = '/tmp/pneumonia_classifier_model'\n",
        "    model.save(TMP_PATH)\n",
        "    print(\"Preparing model for browser download.  Please wait as it could take a while . . . .\")\n",
        "    sleep(2)  # pause for two seconds for the file to save\n",
        "    files.download(TMP_PATH)\n",
        "\n",
        "########################### Google Drive functions #############################\n",
        "# Note that these functions will require interactive steps to create OAuth \n",
        "# tokens that will be used to authorize access to Google Drive\n",
        "\n",
        "# Google Drive path to the saved model\n",
        "GOOGLE_DRIVE_MODEL_PATH = \"/content/drive/My Drive/USU/intelligent_systems/Pneumonia_Classifier/pneumonia_classifier_model\"\n",
        "\n",
        "# mount Google drive\n",
        "def mount_drive():\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# unmount Google drive\n",
        "def unmount_drive():\n",
        "    drive.flush_and_unmount()\n",
        "\n",
        "def save_model_to_google_drive(model):\n",
        "    mount_drive()\n",
        "    model.save(GOOGLE_DRIVE_MODEL_PATH)\n",
        "    unmount_drive()\n",
        "\n",
        "def load_trained_model_from_google_drive():\n",
        "    mount_drive()\n",
        "    model = tf.keras.models.load_model(GOOGLE_DRIVE_MODEL_PATH, custom_objects=None, compile=True)\n",
        "    unmount_drive()\n",
        "    return model\n",
        "\n",
        "\n",
        "###################### Model Training and Evaluation ##############################\n",
        "def train_model():\n",
        "    train_data_gen = create_train_image_generator()\n",
        "    test_data_gen = create_test_image_generator()\n",
        "    model = create_model()\n",
        "    history = model.fit(\n",
        "        train_data_gen,        \n",
        "        steps_per_epoch=train_count // BATCH_SIZE,\n",
        "        epochs=EPOCHS,\n",
        "        validation_data=test_data_gen,        \n",
        "        validation_steps=test_count // BATCH_SIZE\n",
        "    ) \n",
        "    save_model_to_google_drive(model)\n",
        "    return history\n",
        "\n",
        "\n",
        "def train_model_more():\n",
        "    train_data_gen = create_train_image_generator()\n",
        "    test_data_gen = create_test_image_generator()\n",
        "    model = load_trained_model_from_google_drive()\n",
        "    history = model.fit(\n",
        "        train_data_gen,\n",
        "        steps_per_epoch=train_count // BATCH_SIZE,\n",
        "        epochs=EPOCHS,\n",
        "        validation_data=test_data_gen,\n",
        "        validation_steps=test_count // BATCH_SIZE\n",
        "    )    \n",
        "    save_model_to_google_drive(model)\n",
        "    return history\n",
        "    \n",
        "def show_history_metrics(history):\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    epochs_range = range(EPOCHS)\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs_range, loss, label='Training Loss')\n",
        "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.show()\n",
        "\n",
        "def test_trained_model():\n",
        "    test_data_gen = create_test_image_generator()\n",
        "    model = download_trained_model_from_google_drive_shared()\n",
        "    model.evaluate(test_data_gen)\n",
        "\n",
        "def validate_trained_model():\n",
        "    validate_data_gen = create_validate_image_generator()\n",
        "    model = download_trained_model_from_google_drive_shared()\n",
        "    model.evaluate(validate_data_gen)\n",
        "\n",
        "\n",
        "#test_trained_model()\n",
        "history = train_model_more()\n",
        "show_history_metrics(history)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}