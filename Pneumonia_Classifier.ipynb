{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pneumonia_Classifier",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rmcnew/Pneumonia_Classifier/blob/master/Pneumonia_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hfkLTSImBHo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "e6e3fb28-83f2-40d2-af73-c6436a271921"
      },
      "source": [
        "!which gdown"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/bin/gdown\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcAn-1X9Uppw",
        "colab_type": "code",
        "outputId": "f487a10b-f1f9-4f23-c081-e168489bd81d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "# Richard Scott McNew\n",
        "# A02077329\n",
        "# CS 6600: Intelligent Systems\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "from google.colab import drive, files\n",
        "import io\n",
        "import os\n",
        "import pathlib\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# enable accelerated linear algebra\n",
        "tf.config.optimizer.set_jit(True)\n",
        "# enable tensorflow AUTOTUNE\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "IMAGE_SIDE = 450 \n",
        "SHUFFLE_SIZE = 25\n",
        "EPOCHS = 2\n",
        "\n",
        "\n",
        "DATASET_PATH = \"/content/Pneumonia_Classifier/dataset\"\n",
        "\n",
        "def get_dataset_files():\n",
        "    if not os.path.isdir(DATASET_PATH):\n",
        "        print(\"Downloading the dataset . . .\")\n",
        "        !git clone -b dataset_only https://github.com/rmcnew/Pneumonia_Classifier.git \n",
        "    else:\n",
        "        print(\"Using previously downloaded dataset\")\n",
        "\n",
        "\n",
        "# dataset paths\n",
        "get_dataset_files()\n",
        "dataset = pathlib.Path(DATASET_PATH)\n",
        "test = dataset.joinpath(\"test\")\n",
        "test_count = len(list(test.glob('**/*.jpeg')))\n",
        "train = dataset.joinpath(\"train\")\n",
        "train_count = len(list(train.glob('**/*.jpeg')))\n",
        "validate = dataset.joinpath(\"validate\")\n",
        "validate_count = len(list(validate.glob('**/*.jpeg')))\n",
        "\n",
        "####################### Dataset Preprocessing #########################\n",
        "def create_train_image_generator():\n",
        "    train_image_generator = ImageDataGenerator(rescale=1./255, zoom_range=0.5)\n",
        "    train_data_gen = train_image_generator.flow_from_directory(\n",
        "            batch_size=BATCH_SIZE, \n",
        "            directory=str(train), \n",
        "            shuffle=True, \n",
        "            target_size=(IMAGE_SIDE, IMAGE_SIDE), \n",
        "            class_mode='binary')\n",
        "    return train_data_gen\n",
        "\n",
        "def create_test_image_generator():\n",
        "    test_image_generator = ImageDataGenerator(rescale=1./255)\n",
        "    test_data_gen = test_image_generator.flow_from_directory(\n",
        "            batch_size=BATCH_SIZE, \n",
        "            directory=str(test), \n",
        "            target_size=(IMAGE_SIDE, IMAGE_SIDE), \n",
        "            class_mode='binary')\n",
        "    return test_data_gen\n",
        "\n",
        "def create_validate_image_generator():\n",
        "    validate_image_generator = ImageDataGenerator(rescale=1./255)\n",
        "    validate_data_gen = validate_image_generator.flow_from_directory(\n",
        "            batch_size=BATCH_SIZE, \n",
        "            directory=str(validate), \n",
        "            target_size=(IMAGE_SIDE, IMAGE_SIDE), \n",
        "            class_mode='binary')\n",
        "    return validate_data_gen\n",
        "\n",
        "############################ Model Creation and Loading ##############################\n",
        "\n",
        "# val 90\n",
        "#def create_model():\n",
        "#    model = Sequential([\n",
        "#        Conv2D(450, 10, padding='same', activation='relu', kernel_regularizer='l2', input_shape=(450, 450, 3)),\n",
        "#        MaxPooling2D(),\n",
        "#        Conv2D(225, 5, padding='same', activation='relu', kernel_regularizer='l2'),\n",
        "#        MaxPooling2D(),\n",
        "#        Dropout(0.2),\n",
        "#        Conv2D(100, 5, padding='same', activation='relu', kernel_regularizer='l2'),\n",
        "#        MaxPooling2D(),        \n",
        "#        Flatten(),\n",
        "#        Dense(512, activation='relu'),\n",
        "#        Dense(1, activation='sigmoid')\n",
        "#    ])\n",
        "#    model.compile(optimizer='SGD', loss='binary_crossentropy', metrics=['accuracy']) \n",
        "#    return model\n",
        "\n",
        "\n",
        "def create_model():\n",
        "    model = Sequential([\n",
        "        Conv2D(IMAGE_SIDE, 10, padding='same', activation='relu', kernel_regularizer='l2', input_shape=(IMAGE_SIDE, IMAGE_SIDE, 3)),\n",
        "        MaxPooling2D(),\n",
        "        Conv2D(225, 5, padding='same', activation='relu', kernel_regularizer='l2'),\n",
        "        MaxPooling2D(),\n",
        "        Dropout(0.2),\n",
        "        Conv2D(100, 5, padding='same', activation='relu', kernel_regularizer='l2'),\n",
        "        MaxPooling2D(),        \n",
        "        Flatten(),\n",
        "        Dense(512, activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer='SGD', loss='binary_crossentropy', metrics=['accuracy']) \n",
        "    return model\n",
        "\n",
        "    \n",
        "# There is a trained persisted model that is shared from my Google Drive account\n",
        "# Download a copy to local storage for use in evaluating against the datasets\n",
        "def download_trained_model_from_google_drive_shared():\n",
        "    print(\"Downloading trained model from Google Drive . . .\")\n",
        "    !gdown https://drive.google.com/uc?id=1Qwm5QlveZsUzO6vU7jMBUOgx_uHlSN9i\n",
        "    print(\"Download completed!  Loading the model . . .\")\n",
        "    model = tf.keras.models.load_model(\"/content/pneumonia_classifier_model\", custom_objects=None, compile=True)\n",
        "    print(\"Model loaded.\")\n",
        "    return model\n",
        "\n",
        "\n",
        "########################### Google Drive functions #############################\n",
        "# Note that these functions will require interactive steps to create OAuth \n",
        "# tokens that will be used to authorize access to Google Drive\n",
        "\n",
        "# Google Drive path to the saved model\n",
        "MODEL_PATH = \"/content/drive/My Drive/USU/intelligent_systems/Pneumonia_Classifier/pneumonia_classifier_model\"\n",
        "\n",
        "# mount Google drive\n",
        "def mount_drive():\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# unmount Google drive\n",
        "def unmount_drive():\n",
        "    drive.flush_and_unmount()\n",
        "\n",
        "def save_model_to_google_drive(model):\n",
        "    mount_drive()\n",
        "    model.save(MODEL_PATH)\n",
        "    unmount_drive()\n",
        "\n",
        "def load_trained_model_from_google_drive():\n",
        "    mount_drive()\n",
        "    model = tf.keras.models.load_model(MODEL_PATH, custom_objects=None, compile=True)\n",
        "    unmount_drive()\n",
        "    return model\n",
        "\n",
        "\n",
        "###################### Model Training and Evaluation ##############################\n",
        "def train_model():\n",
        "    train_data_gen = create_train_image_generator()\n",
        "    test_data_gen = create_test_image_generator()\n",
        "    model = create_model()\n",
        "    model.fit(\n",
        "        train_data_gen,        \n",
        "        steps_per_epoch=train_count // BATCH_SIZE,\n",
        "        epochs=EPOCHS,\n",
        "        validation_data=test_data_gen,        \n",
        "        validation_steps=test_count // BATCH_SIZE\n",
        "    ) \n",
        "    save_model_to_google_drive(model)\n",
        "\n",
        "\n",
        "def train_model_more():\n",
        "    train_data_gen = create_train_image_generator()\n",
        "    test_data_gen = create_test_image_generator()\n",
        "    model = load_trained_model_from_google_drive()\n",
        "    model.fit(\n",
        "        train_data_gen,\n",
        "        steps_per_epoch=train_count // BATCH_SIZE,\n",
        "        epochs=EPOCHS,\n",
        "        validation_data=test_data_gen,\n",
        "        validation_steps=test_count // BATCH_SIZE\n",
        "    )    \n",
        "    save_model_to_google_drive(model)\n",
        "    \n",
        "def evaluate_trained_model():\n",
        "    test_data_gen = create_test_image_generator()\n",
        "    model = download_trained_model_from_google_drive_shared()\n",
        "    model.evaluate(test_data_gen)\n",
        "\n",
        "\n",
        "evaluate_trained_model()\n",
        "#train_model_more()\n",
        "#unmount_drive()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using previously downloaded dataset\n",
            "Found 624 images belonging to 2 classes.\n",
            "Downloading trained model from Google Drive . . .\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Qwm5QlveZsUzO6vU7jMBUOgx_uHlSN9i\n",
            "To: /content/pneumonia_classifier_model\n",
            "655MB [00:05, 116MB/s]\n",
            "Download completed!  Loading the model . . .\n",
            "Model loaded.\n",
            "78/78 [==============================] - 18s 236ms/step - loss: 0.3489 - acc: 0.8846\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}