{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pneumonia_Classifier",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rmcnew/Pneumonia_Classifier/blob/master/Pneumonia_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDZqRGr729uL",
        "colab_type": "text"
      },
      "source": [
        "#**Pneumonia Classifer**\n",
        "Richard Scott McNew\n",
        "\n",
        "A02077329\n",
        "\n",
        "CS 6600:  Intelligent Systems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJDh0O2Qa17K",
        "colab_type": "text"
      },
      "source": [
        "##Objective\n",
        "\n",
        "Develop and train an image classification network to predict a pneumonia diagnosis when given a chest X-ray image. Use tools other than tflearn in order to explore and learn how to employ other machine learning tools."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39_Z3RuB3tkl",
        "colab_type": "text"
      },
      "source": [
        "##Motivation\n",
        "Pneumonia is the number one killer of children under 5 years old world-wide, killing about 2 million children every year (Rudan et al., 2008).  It is estimated that pediatric pneumonia kills more children than HIV/AIDS, malaria, and measles combined (Adegbola, 2012).  \n",
        "\n",
        "Practically all of these pediatric pneumonia cases occur in developing countries throughout Southeast Asia and Africa.  Pediatric pneumonia can be caused by both baterical or viral pathogens (Mcluckie, 2009).  Timely and accurate diagnosis and follow-on treatment are essential to ensure the survival of affected children. \n",
        "\n",
        "Chest X-rays are a primary means of diagnosing pneumonia, but rapid interpretation of chest X-rays are frequently not available in the developing nations where pediatric pneumonia is common and mortality rates are high.\n",
        "\n",
        "This Pneumonia Classifier takes a chest X-ray image as input and provides a high-confidence automated diagnosis as to whether the patient has pneumonia or not.\n",
        "\n",
        "![Two Chest X-Rays Compared:  a normal chest X-ray and a pneumonia chest X-ray](https://drive.google.com/uc?export=view&id=14kjhca9G-0et78rcxbX4XEh_oyC-RlBl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oan0AUAtcdVB",
        "colab_type": "text"
      },
      "source": [
        "##Dataset\n",
        "The chest X-ray dataset consists of 5856 chest X-ray images of various dimensions.  Images are labelled as one of two sets: NORMAL or PNEUMONIA.\n",
        "\n",
        "Dataset Source URL: https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia/\n",
        "\n",
        "\n",
        "### Dataset Image Counts\n",
        "![Table showing the number of images in each dataset subset](https://drive.google.com/uc?export=view&id=1W-QYvSI4d69j5crKpYw-X0tjzg40yI_6)\n",
        "\n",
        "### Dataset Image Preprocessing\n",
        "The dataset images must be preprocessed before they are ready for use in training the convolution networks.  The following preprocessing actions are taken for all of the images before they are processed:\n",
        "* Resized to 450 x 450 dimensions\n",
        "* Numpy array values for each pixel are rescaled to [0, 1] range\n",
        "\n",
        "The Train dataset subset images also have the following transformations randomly applied to compensate for the relatively small number of dataset images (dataset augmentation) and to help prevent overfitting:\n",
        "* Images may be randomly rotated up to plus or minus 10 degrees (this is given by the *rotation_range* parameter)\n",
        "* Images may be randomly translated horizontally or vertically up to 10% of the image width or height (this is given in the *width_shift* and *height_shift* parameters)\n",
        "* Images may be randomly shear transformed up to 5% (this is given in the *shear_range* parameter)\n",
        "* Images may be randomly zoomed to only show up to 20% portion of the original image (this is given in the *zoom_range* parameter)\n",
        "\n",
        "The following images show the how randomly applying the above transformations to training images adds variety to the input data:\n",
        "![Images of cats with transforms applied to give variety to the input training dataset](https://drive.google.com/uc?export=view&id=14sJaDMApd5Po7cIZQsIZK03eGbdydA-5)Image source: https://blog.keras.io/img/imgclf/cat_data_augmentation.png\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aYL1p5Lc5hN",
        "colab_type": "text"
      },
      "source": [
        "##Tooling\n",
        "\n",
        "###Google Colaboratory Advantages\n",
        "\n",
        "After some exploration, I choose to use Google Colaboratory as the primary tool to train the Convolution Nets.  This was largely motivated by the limited hardware that I have available to perform convolution net training.  Google Colaboratory offers a no-cost, powerful GPU-accelerated virtual machine with more RAM than my laptop (25 GB RAM  on the virtual machine versus 16 GB RAM on my laptop).\n",
        "\n",
        "This meant that I could train convolution nets much faster and get feedback on my design decisions more rapidly than I could with my laptop.  This screenshot shows a training run with the same dataset and convolution net hyperparameters running on a Google Colaboratory instance and my laptop.  (The Google Colaboratory instance is above.  My laptop is below.)\n",
        "\n",
        "![Screenshot showing Colaboratry instance running compared to my laptop running.  Colaboratory training run estimates 1 hour and 4 minutes left to finish the training while my laptop shows 5 hours and 38 minutes left to finish the training.](https://drive.google.com/uc?export=view&id=14i6Tys9WkG8B2Akn5iw9zDIU1uUS4N8R)\n",
        "\n",
        "Note that the Colaboratory instance is completing training epoch 1 much faster than my laptop.  The Colaboratory instance estimates one hour and four minutes left compared to the five hours and thirty-eight minutes left on my laptop.  This is due to a faster virtual machine processor, more RAM, and GPU-acceleration.\n",
        "\n",
        "Google Colaboratory offers a ready-to-go Python-powered machine learning platform based on Jupyter Notebook that requires practically no setup and runs in the web browser.  The pre-installed libraries are Tensorflow-centric, but also include popular Python numerical (numpy, scipy), image processing (opencv, scikit-image), graph visualization (matplotlib, plotly), and machine learning (tensorflow, keras, tflearn, pandas, scikit-learn, pytorch) libraries.  This makes it possible to do a wide range of data transformation and visualization without extensive setup. \n",
        "\n",
        "###Google Colaboratory Disadvantages\n",
        "Google Colaboratory instances on the no-cost tier are intended primarily for interactive use and have a timeout of 90 minutes, meaning that the Colaboratory notebook will be automatically disconnected from a backend virtual machine if no activity occurs in the notebook.  This effectively prevents brute-force searches of the hyperparameters since it means that long-running training of multiple epochs is more difficult to accomplish without \"baby-sitting\" the notebook.  I found this to not be too disadvantageous since it encouraged me to monitor the training more closely and notice immediately when signs of overfitting began to be manifest.\n",
        "\n",
        "Although it does not occur very often, sometimes the Google Colaboratory virtual machine instances can be flaky and will exhibit transitory failures.  This usually manifests as out-of-memory failures or much slower training times.  In these cases, it was best to either restart the runtime (\"Runtime\" -> \"Restart Runtime\" in the Menu) or disconnect from the \"lemon\" virtual machine backend and let the session timeout.  Following the restart and connecting to a different virtual machine backend the problems disappear and no changes are needed to the code being run.  If Out of Memory errors continue to occur, this usually means that the model or dataset input data is too large and needs to be scaled down.\n",
        "\n",
        "###Machine Learning Framework Choice\n",
        "The Keras APIs that are included with Tensorflow 2.X are used for image processing, model creation, and model training.  Tensorflow 2.x and Keras are well-supported on Google Colaboratory and take can advantage of GPU acceleration for rapid model training. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLzXiRilcpTS",
        "colab_type": "text"
      },
      "source": [
        "##Code for Training and Testing the Convolution Nets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcAn-1X9Uppw",
        "colab_type": "code",
        "outputId": "d39bf204-6ec6-4015-faf2-1750e5256715",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        }
      },
      "source": [
        "# Richard Scott McNew\n",
        "# A02077329\n",
        "# CS 6600: Intelligent Systems\n",
        "\n",
        "# use tensorflow 2.x\n",
        "%tensorflow_version 2.x\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "from google.colab import drive, files\n",
        "import datetime\n",
        "import io\n",
        "import os\n",
        "import pathlib\n",
        "from time import sleep\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# enable accelerated linear algebra\n",
        "tf.config.optimizer.set_jit(True)\n",
        "# enable tensorflow AUTOTUNE\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "# Load the TensorBoard notebook extension for training metrics graphs\n",
        "%load_ext tensorboard\n",
        "\n",
        "\n",
        "################### Constants #######################\n",
        "BATCH_SIZE = 8  # Use small batches to allow current batch to fit in GPU memory for faster training\n",
        "IMAGE_SIDE = 450 # We have to resize the dataset images to reduce memory consumption\n",
        "SHUFFLE_SIZE = 25 # We shuffle the images to ensure a better fit \n",
        "EPOCHS = 2  # Only run a few epochs at a time since Colaboratory times out without interactive use\n",
        "\n",
        "# ensure we are in the correct working directory\n",
        "os.chdir(\"/content\")\n",
        "\n",
        "######### Dataset Download and Path Construction #####################\n",
        "# Local path to the dataset\n",
        "DATASET_PATH = \"/content/Pneumonia_Classifier/dataset\"\n",
        "\n",
        "# There is a copy of the Pneumonia dataset in my Pneumonia_Classifier GitHub repo\n",
        "# We can clone the 'dataset_only' branch to get a local copy\n",
        "def get_dataset_files_from_github():\n",
        "    if not os.path.isdir(DATASET_PATH):\n",
        "        print(\"Downloading the dataset from GitHub . . .\")\n",
        "        !git clone -b dataset_only https://github.com/rmcnew/Pneumonia_Classifier.git \n",
        "    else:\n",
        "        print(\"Using previously downloaded dataset\")\n",
        "\n",
        "# There is a tarball of the Pneumonia dataset available as a publicly shared link\n",
        "# from my Google Drive account.  This is probably the fastest way to download a \n",
        "# local copy of the dataset since it should be all within Google's networks\n",
        "def get_dataset_files_from_google_drive_shared():\n",
        "    if not os.path.isdir(DATASET_PATH):\n",
        "        print(\"Downloading the dataset from Google Drive shared link . . .\")\n",
        "        !gdown https://drive.google.com/uc?id=1u2_Ap4rOxHuEKnSb5te070skuoXcJTX9\n",
        "        print(\"Download completed!  Untarring the dataset . . .\")\n",
        "        !tar xjf Pneumonia_Classifier_dataset.tar.bz2\n",
        "        print(\"Dataset is ready!\")\n",
        "    else:\n",
        "        print(\"Using previously downloaded dataset\")\n",
        "\n",
        "# Download a local copy of the dataset and then build paths \n",
        "# to the different dataset subsets: 'train', 'test', and 'validate'\n",
        "#get_dataset_files_from_github()\n",
        "get_dataset_files_from_google_drive_shared()\n",
        "dataset = pathlib.Path(DATASET_PATH)\n",
        "test = dataset.joinpath(\"test\")\n",
        "test_count = len(list(test.glob('**/*.jpeg')))\n",
        "train = dataset.joinpath(\"train\")\n",
        "train_count = len(list(train.glob('**/*.jpeg')))\n",
        "validate = dataset.joinpath(\"validate\")\n",
        "validate_count = len(list(validate.glob('**/*.jpeg')))\n",
        "\n",
        "\n",
        "####################### Dataset Preprocessing #########################\n",
        "# The train_image_generator applies random transformations to the \n",
        "# Train dataset subset to do dataset augmentation since the dataset\n",
        "# is small and we want to avoid overfitting\n",
        "def create_train_image_generator():\n",
        "    train_image_generator = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        rotation_range=10,\n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1, \n",
        "        zoom_range=0.2, \n",
        "        shear_range=0.05,\n",
        "        fill_mode='nearest')\n",
        "    train_data_gen = train_image_generator.flow_from_directory(\n",
        "            batch_size=BATCH_SIZE, \n",
        "            directory=str(train), \n",
        "            shuffle=True, \n",
        "            target_size=(IMAGE_SIDE, IMAGE_SIDE), \n",
        "            class_mode='categorical')\n",
        "    return train_data_gen\n",
        "\n",
        "# Prepare images in the Test dataset subset for use\n",
        "def create_test_image_generator():\n",
        "    test_image_generator = ImageDataGenerator(rescale=1./255)\n",
        "    test_data_gen = test_image_generator.flow_from_directory(\n",
        "            batch_size=BATCH_SIZE, \n",
        "            directory=str(test), \n",
        "            target_size=(IMAGE_SIDE, IMAGE_SIDE), \n",
        "            class_mode='categorical')\n",
        "    return test_data_gen\n",
        "\n",
        "# Prepare images in the Validate dataset subset for use\n",
        "def create_validate_image_generator():\n",
        "    validate_image_generator = ImageDataGenerator(rescale=1./255)\n",
        "    validate_data_gen = validate_image_generator.flow_from_directory(\n",
        "            batch_size=BATCH_SIZE, \n",
        "            directory=str(validate), \n",
        "            target_size=(IMAGE_SIDE, IMAGE_SIDE), \n",
        "            class_mode='categorical')\n",
        "    return validate_data_gen\n",
        "\n",
        "\n",
        "############################ Model Creation, Loading, and Saving ##############################\n",
        "### This model tends to approach the upper limit of available memory\n",
        "### Larger models with more layers or greater numbers of features run into Out of Memory errors\n",
        "\n",
        "def create_model():\n",
        "    model = Sequential([\n",
        "        Conv2D(IMAGE_SIDE, 15, padding='same', activation='relu', kernel_regularizer='l2', \n",
        "               input_shape=(IMAGE_SIDE, IMAGE_SIDE, 3)),\n",
        "        MaxPooling2D(),\n",
        "        Dropout(0.2),\n",
        "        Conv2D(225, 10, padding='same', activation='relu', kernel_regularizer='l2'),\n",
        "        MaxPooling2D(),\n",
        "        Dropout(0.2),\n",
        "        Conv2D(100, 5, padding='same', activation='relu', kernel_regularizer='l2'),\n",
        "        MaxPooling2D(),\n",
        "        Flatten(),\n",
        "        Dense(512, activation='relu', kernel_regularizer='l2'),\n",
        "        Dense(2, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy']) \n",
        "    return model\n",
        "\n",
        "###### Trained model from my Google Drive shared link #######\n",
        "# There is the trained persisted model that is shared from my Google Drive account.\n",
        "# This model can be downloaded by anyone who has the shared link URL.\n",
        "# The 'gdown' command line tool is used to perform non-interactive downloads.\n",
        "# This is the 'final' trained model that is trained to be as accurate as possible.\n",
        "\n",
        "# Download a copy of the trained model to evaluate it against the Test or Validate\n",
        "# dataset subsets or to predict using an uploaded chest X-ray image\n",
        "def download_trained_model_from_google_drive_shared(download_anyway=False):\n",
        "    MODEL_PATH = \"/content/pneumonia_classifier_model.h5\"\n",
        "    if not os.path.exists(MODEL_PATH) or download_anyway:\n",
        "        print(\"Downloading trained model from Google Drive shared link . . .\")\n",
        "        !gdown https://drive.google.com/uc?id=1wOB-6Tn4-kexFliYaoO42Qvfneih81g7\n",
        "        print(\"Download completed!  Loading the model . . .\")\n",
        "    else:\n",
        "        print(\"Using previously downloaded model\")\n",
        "    model = tf.keras.models.load_model(MODEL_PATH, custom_objects=None, compile=True)\n",
        "    print(\"Model loaded.\")\n",
        "    return model\n",
        "\n",
        "# Save the trained model and then create a web browser dialogue to download it\n",
        "# This can take a bit to run and download since the model occupies about 700 MB on disk\n",
        "def download_trained_model_via_browser(model):\n",
        "    TMP_PATH = '/tmp/pneumonia_classifier_model.h5'\n",
        "    model.save(TMP_PATH, overwrite=True, include_optimizer=True, save_format='h5')\n",
        "    print(\"Preparing model for browser download.  Please wait as it could take a while . . . .\")\n",
        "    sleep(2)  # pause for two seconds for the file to save\n",
        "    files.download(TMP_PATH)\n",
        "\n",
        "\n",
        "########################### Google Drive functions #############################\n",
        "# These functions are only needed if you want to load a trained network from your\n",
        "# Google Drive storage or save a trained network to your Google Drive storage.\n",
        "\n",
        "# Note that these functions will require interactive steps (opening a page in a \n",
        "# web browser and copying / pasting the authoriztaion code) to create OAuth \n",
        "# tokens that will be used to authorize access to your Google Drive account.\n",
        "# The URL to authorize access is given in the output of this cell (at the bottom)\n",
        "# when these functions are called to access Google Drive.\n",
        "\n",
        "# mount Google drive\n",
        "def mount_drive():\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# unmount Google drive\n",
        "def unmount_drive():\n",
        "    drive.flush_and_unmount()\n",
        "\n",
        "# Google Drive path to the saved model\n",
        "# Note that you may encounter an error if this path does not exist in your Google Drive account\n",
        "GOOGLE_DRIVE_MODEL_PATH = \"/content/drive/My Drive/USU/intelligent_systems/Pneumonia_Classifier/pneumonia_classifier_model.h5\"\n",
        "\n",
        "def save_model_to_google_drive(model):\n",
        "    print(\"Saving model to Google Drive\")\n",
        "    mount_drive()\n",
        "    model.save(GOOGLE_DRIVE_MODEL_PATH, overwrite=True, include_optimizer=True, save_format='h5')    \n",
        "    unmount_drive()\n",
        "\n",
        "def load_trained_model_from_google_drive():\n",
        "    print(\"Loading saved model from Google Drive\")\n",
        "    mount_drive()\n",
        "    model = tf.keras.models.load_model(GOOGLE_DRIVE_MODEL_PATH, custom_objects=None, compile=True)    \n",
        "    return model\n",
        "\n",
        "\n",
        "###################### Model Training and Evaluation ##############################\n",
        "# Create the initial version of the model and run some training epochs\n",
        "# After training epochs run, save the trained model to Google Drive or download it\n",
        "def train_model():\n",
        "    print(\"Training model from scratch\")\n",
        "    print(\"Preparing Train and Test dataset subsets\")\n",
        "    train_data_gen = create_train_image_generator()\n",
        "    test_data_gen = create_test_image_generator()\n",
        "    model = create_model()    \n",
        "    log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "    print(\"Training model . . .\")\n",
        "    history = model.fit(\n",
        "        train_data_gen,        \n",
        "        steps_per_epoch=train_count // BATCH_SIZE,\n",
        "        epochs=EPOCHS,\n",
        "        validation_data=test_data_gen,        \n",
        "        validation_steps=test_count // BATCH_SIZE,\n",
        "        callbacks=[tensorboard_callback]\n",
        "    ) \n",
        "    save_model_to_google_drive(model)\n",
        "\n",
        "\n",
        "# Load the previously trained model from Google Drive, run more training \n",
        "# epochs, and then save the more trained model back to Google Drive\n",
        "def train_model_more():\n",
        "    print(\"Training model more\")\n",
        "    print(\"Preparing training and testing datasets\")\n",
        "    train_data_gen = create_train_image_generator()\n",
        "    test_data_gen = create_test_image_generator()\n",
        "    model = load_trained_model_from_google_drive()    \n",
        "    log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "    print(\"Training model more . . .\")\n",
        "    history = model.fit(\n",
        "        train_data_gen,\n",
        "        steps_per_epoch=train_count // BATCH_SIZE,\n",
        "        epochs=EPOCHS,\n",
        "        validation_data=test_data_gen,\n",
        "        validation_steps=test_count // BATCH_SIZE,\n",
        "        callbacks=[tensorboard_callback]\n",
        "    )    \n",
        "    save_model_to_google_drive(model)\n",
        "    \n",
        "# Download the trained model from the Google Drive shared link\n",
        "# and then evaluate the trained model's accuracy against the \n",
        "# Test dataset subset    \n",
        "def test_trained_model():\n",
        "    print(\"Running model against Test dataset subset\")\n",
        "    test_data_gen = create_test_image_generator()\n",
        "    model = download_trained_model_from_google_drive_shared()\n",
        "    model.evaluate(test_data_gen)\n",
        "\n",
        "# Download the trained model from the Google Drive shared link\n",
        "# and then evaluate the trained model's accuracy against the \n",
        "# Validate dataset subset\n",
        "def validate_trained_model():\n",
        "    print(\"Running model against Validate dataset subset\")\n",
        "    validate_data_gen = create_validate_image_generator()\n",
        "    model = download_trained_model_from_google_drive_shared()\n",
        "    model.evaluate(validate_data_gen)\n",
        "\n",
        "\n",
        "############ Main Section ###############\n",
        "# These function calls will need to be \n",
        "# commented out or uncommented depending \n",
        "# on what you want to do \n",
        "\n",
        "#train_model()\n",
        "train_model_more()\n",
        "#test_trained_model()\n",
        "#validate_trained_model()\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n",
            "Using previously downloaded dataset\n",
            "Training model from scratch\n",
            "Preparing Train and Test dataset subsets\n",
            "Found 5216 images belonging to 2 classes.\n",
            "Found 624 images belonging to 2 classes.\n",
            "Training model . . .\n",
            "Train for 652 steps, validate for 78 steps\n",
            "Epoch 1/2\n",
            "652/652 [==============================] - 1769s 3s/step - loss: 13.5030 - accuracy: 0.7408 - val_loss: 11.9654 - val_accuracy: 0.6250\n",
            "Epoch 2/2\n",
            "652/652 [==============================] - 1767s 3s/step - loss: 10.4963 - accuracy: 0.7387 - val_loss: 9.2856 - val_accuracy: 0.6250\n",
            "Saving model to Google Drive\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9CfYE38faJ_",
        "colab_type": "text"
      },
      "source": [
        "#### TensorBoard Graphs\n",
        "TensorBoard graphs can be used to visualize the convolution network training metrics and determine if any changes need to be made.  The graphs are derived from training metric logs which are enabled via *tensorboard_callback* functions above.\n",
        "\n",
        "Running the cells below will build dynamic graphs from the previous training runs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMVw3sIkim3y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Delete training metric logs from previous training runs\n",
        "# These logs are used by TensorBoard to create the training metrics graphs\n",
        "# so DO NOT run this unless you want to delete the old logs.\n",
        "!rm -rf ./logs/ "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNGYB3QiiI5H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run me to see the training metric graphs for the convolution \n",
        "# net training that you just ran.  Note that the logs are created\n",
        "# on the virtual machine backend and are not persisted unless you\n",
        "# do something to keep the training metric logs.\n",
        "\n",
        "# For some reason TensorBoard needs to be run in its own cell, so \n",
        "# this cell is apart from the rest of the convolution net training\n",
        "# code above\n",
        "%tensorboard --logdir logs/fit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrP4clbrbIaJ",
        "colab_type": "text"
      },
      "source": [
        "### Upload and Run a Chest X-Ray Image Against the Trained Model\n",
        "Run the following code block to upload a chest X-ray image from your web browser and get a pneumonia prediction.  \n",
        "\n",
        "Use the \"Choose Files\" button in the output pane to select an image file to upload.  If you get the error message, \"Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable.\" then re-run the code block and it should succeed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-1Q2dXWrK9o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "from google.colab import files\n",
        "import os\n",
        "import shutil\n",
        "import tempfile\n",
        "from time import sleep\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "IMAGE_SIDE = 450 # We have to resize the dataset images to reduce memory consumption\n",
        "\n",
        "def download_trained_model_from_google_drive_shared(download_anyway=False):\n",
        "    MODEL_PATH = \"/content/pneumonia_classifier_model.h5\"\n",
        "    if not os.path.exists(MODEL_PATH) or download_anyway:\n",
        "        print(\"Downloading trained model from Google Drive shared link . . .\")\n",
        "        !gdown https://drive.google.com/uc?id=1wOB-6Tn4-kexFliYaoO42Qvfneih81g7\n",
        "        print(\"Download completed!  Loading the model . . .\")\n",
        "    else:\n",
        "        print(\"Using previously downloaded model\")\n",
        "    model = tf.keras.models.load_model(MODEL_PATH, custom_objects=None, compile=True)\n",
        "    print(\"Model loaded.\")\n",
        "    return model\n",
        "\n",
        "def upload_images_and_save_files():\n",
        "    files.upload()\n",
        "\n",
        "# create an image generator for the uploaded images\n",
        "def create_image_generator(dir_name):\n",
        "    print(\"Creating image generator for images in directory: \", dir_name)\n",
        "    image_generator = ImageDataGenerator(rescale=1./255)\n",
        "    data_gen = image_generator.flow_from_directory(\n",
        "            directory=str(dir_name), \n",
        "            target_size=(IMAGE_SIDE, IMAGE_SIDE), \n",
        "            class_mode=None) # class_mode has to be None to indicate there are no data labels\n",
        "    return data_gen\n",
        "  \n",
        "# Round the prediction to a human-readable interpretation\n",
        "def interpret_prediction(prediction):\n",
        "    pred_list = prediction[0].tolist()\n",
        "    max_index = pred_list.index(max(pred_list))\n",
        "    if max_index == 0:\n",
        "        print(\"NORMAL\")  # these might need to be swapped\n",
        "    else:\n",
        "        print(\"PNEUMONIA\")\n",
        "\n",
        "##### Main section #####\n",
        "model = download_trained_model_from_google_drive_shared()\n",
        "# create temporary directory\n",
        "temp_dir_name = tempfile.mkdtemp()\n",
        "print('Created temporary directory: ', temp_dir_name)\n",
        "# cd to temporary directory\n",
        "os.chdir(temp_dir_name)\n",
        "# make subdirectory for image files;  per the Keras documentation:  \n",
        "# \"Please note that in case of class_mode None, the data still needs \n",
        "# to reside in a subdirectory of directory for it to work correctly.\"\n",
        "image_path = os.path.join(temp_dir_name, \"images\")\n",
        "os.mkdir(image_path)\n",
        "os.chdir(image_path)\n",
        "# upload image files from user's web browser into image_path\n",
        "upload_images_and_save_files()\n",
        "sleep(2) # pause to save files to disk\n",
        "# preprocess uploaded images\n",
        "image_gen = create_image_generator(temp_dir_name)\n",
        "# run prediction\n",
        "prediction = model.predict(image_gen)\n",
        "# interpret and print out result\n",
        "interpret_prediction(prediction)\n",
        "# return to starting directory\n",
        "os.chdir(\"/content\")\n",
        "# clean-up temporary directory\n",
        "shutil.rmtree(temp_dir_name)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYKJE3oyXE1I",
        "colab_type": "text"
      },
      "source": [
        "##Performance Results\n",
        "The following shows how the trained models performed against the Test and Validate dataset subsets:\n",
        "\n",
        "#***Table of Performance Results goes here***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dE0gScnpupw4",
        "colab_type": "text"
      },
      "source": [
        "##Download Trained Model\n",
        "Run this cell to download the trained model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miPFGzI5tDI6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###### Trained model from my Google Drive shared link #######\n",
        "# There is the trained persisted model that is shared from my Google Drive account.\n",
        "# This model can be downloaded by anyone who has the shared link URL.\n",
        "# The 'gdown' command line tool is used to perform non-interactive downloads.\n",
        "# This is the 'final' trained model that is trained to be as accurate as possible.\n",
        "\n",
        "# Download a copy of the trained model to evaluate it against the Test or Validate\n",
        "# dataset subsets or to predict using an uploaded chest X-ray image\n",
        "def download_trained_model_from_google_drive_shared(download_anyway=False):\n",
        "    MODEL_PATH = \"/content/pneumonia_classifier_model.h5\"\n",
        "    if not os.path.exists(MODEL_PATH) or download_anyway:\n",
        "        print(\"Downloading trained model from Google Drive shared link . . .\")\n",
        "        !gdown https://drive.google.com/uc?id=1wOB-6Tn4-kexFliYaoO42Qvfneih81g7\n",
        "        print(\"Download completed!  Loading the model . . .\")\n",
        "    else:\n",
        "        print(\"Using previously downloaded model\")\n",
        "    model = tf.keras.models.load_model(MODEL_PATH, custom_objects=None, compile=True)\n",
        "    print(\"Model loaded.\")\n",
        "    return model\n",
        "\n",
        "# Save the trained model and then create a web browser dialogue to download it\n",
        "# This can take a bit to run and download since the model occupies about 700 MB on disk\n",
        "def download_trained_model_via_browser(model):\n",
        "    TMP_PATH = '/tmp/pneumonia_classifier_model.h5'\n",
        "    model.save(TMP_PATH, overwrite=True, include_optimizer=True, save_format='h5')\n",
        "    print(\"Preparing model for browser download.  Please wait as it could take a while . . . .\")\n",
        "    sleep(2)  # pause for two seconds for the file to save\n",
        "    files.download(TMP_PATH)\n",
        "\n",
        "# Main section\n",
        "model = download_trained_model_from_google_drive_shared(download_anyway=True)\n",
        "download_trained_model_via_browser(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_mLanki7CHJ",
        "colab_type": "text"
      },
      "source": [
        "##References\n",
        "Kermany et al., 2018.  \"Identifying Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning\", Cell 172, p. 1122–1131, February 22, 2018.  Elsevier Inc.\n",
        "https://doi.org/10.1016/j.cell.2018.02.010\n",
        "\n",
        "\n",
        "Rudan, I., Boschi-Pinto, C., Biloglav, Z., Mulholland, K., and Campbell, H. (2008). \"Epidemiology and Etiology of Childhood Pneumonia\" Bulletin of the World Health Organization 86, 408–416.\n",
        "\n",
        "\n",
        "Adegbola, R.A. (2012). \"Childhood Pneumonia as a Global Health Priority and the Strategic Interest of the Bill & Melinda Gates Foundation\". Clinical Infectious Diseases 54\n",
        "(Supplement 2), S89–S92.\n",
        "\n",
        "\n",
        "Mcluckie, A. (2009). \"Respiratory disease and its management\", Volume 57 (Springer)."
      ]
    }
  ]
}